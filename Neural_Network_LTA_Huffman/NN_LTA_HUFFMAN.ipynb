{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STA/LTA: Detección de eventos por medio de ventanas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafa/anaconda3/lib/python3.11/site-packages/obspy/signal/filter.py:62: UserWarning: Selected high corner frequency (10) of bandpass is at or above Nyquist (3.3125). Applying a high-pass instead.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesamiento completo. Los resultados se guardaron en resultados_eventos_relativos.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from obspy import read\n",
    "from obspy.signal.trigger import classic_sta_lta, trigger_onset\n",
    "\n",
    "root_directory = \"tests\"\n",
    "output_csv = \"resultados_eventos_relativos.csv\"\n",
    "\n",
    "\n",
    "with open(output_csv, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"filename\", \"time_rel(sec)\"])\n",
    "    for subdir, dirs, files in os.walk(root_directory):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(\".mseed\"):\n",
    "                file_path = os.path.join(subdir, file_name)\n",
    "                \n",
    "        \n",
    "                st = read(file_path)\n",
    "                tr = st[0]\n",
    "\n",
    "                # Filtrar la señal\n",
    "                tr_filt = tr.filter(\"bandpass\", freqmin=1, freqmax=10)\n",
    "                \n",
    "                # STA/LTA\n",
    "                sta = 3  # Ventana corta \n",
    "                lta = 50  # Ventana larga\n",
    "                cft = classic_sta_lta(tr_filt.data, int(sta * tr_filt.stats.sampling_rate), int(lta * tr_filt.stats.sampling_rate))\n",
    "                on_trig = 3.5  # Umbral-activación\n",
    "                off_trig = 0.5  # Umbral-desactivación\n",
    "                triggers = trigger_onset(cft, on_trig, off_trig)\n",
    "                for event in triggers:\n",
    "                    inicio_relativo = event[0] / tr_filt.stats.sampling_rate  \n",
    "                    fin_relativo = event[1] / tr_filt.stats.sampling_rate   \n",
    "                    t = (inicio_relativo + fin_relativo) / 2\n",
    "                    writer.writerow([file_name, str(t)])\n",
    "print(f\"Procesamiento completo!!!! Los resultados se guardaron en {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder: Reducción de datos por medio de extracción de features con una Red Neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafa/anaconda3/lib/python3.11/site-packages/obspy/signal/filter.py:62: UserWarning: Selected high corner frequency (10) of bandpass is at or above Nyquist (10.0). Applying a high-pass instead.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 201ms/step - loss: 0.0020\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0020\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0020\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0020\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0020\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0020\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0020\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0020\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0020\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0020\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0020\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0020\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0020\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0019\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0019\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0019\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0019\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0019\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0019\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0019\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0019\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0019\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0019\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0019\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0019\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0019\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0019\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0019\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0019\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0018\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0018\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0018\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0018\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0018\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0018\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0018\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0018\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0018\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0018\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0018\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0018\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0018\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0018\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0017\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0017\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0017\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0017\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0017\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0017\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0017\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0017\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0017\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0017\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0017\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0017\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0017\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0017\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0016\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0016\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0016\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0016\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0016\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0016\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0016\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0016\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0016\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0016\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0016\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0016\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0016\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0015\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0015\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0015\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0015\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0015\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0015\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0015\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0015\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0015\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0015\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0015\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0015\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0015\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0014\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0014\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0014\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0014\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0014\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0014\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0014\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0014\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0014\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0014\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0014\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0014\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0014\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0014\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0013\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0013\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0013\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "Error de reconstrucción: 0.03343262441397872\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from obspy import read\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "st = read(\"XB.ELYSE.02.BHV.2022-02-03HR08_evid0005.mseed\")\n",
    "data = st[0].data  # Suponiendo que tienes una sola traza\n",
    "tr = st[0]  \n",
    "# Filtrar la señal (banda de paso entre 1-10 Hz)\n",
    "tr_filt = tr.filter(\"bandpass\", freqmin=1, freqmax=10)\n",
    "\n",
    "# Definir la arquitectura del autoencoder\n",
    "input_dim = tr_filt.data.shape[0]\n",
    "encoding_dim = 32 \n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "# Crear el autoencoder\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# Compilar el autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error') # MSE para la reconstrucción\n",
    "\n",
    "# Normalizar los datos\n",
    "scaler = MinMaxScaler()\n",
    "signal_data = tr_filt.data.reshape(-1, 1) # ---> para normalizar\n",
    "signal_data_normalized = scaler.fit_transform(signal_data).reshape(1, -1)\n",
    "\n",
    "# Entrenar el autoencoder con la señal del evento normalizada\n",
    "autoencoder.fit(signal_data_normalized, signal_data_normalized, epochs=100, batch_size=128, shuffle=False)\n",
    "\n",
    "# Usar el autoencoder para detectar si nuevas señales son similares al evento\n",
    "reconstructed_signal = autoencoder.predict(signal_data_normalized)\n",
    "reconstruction_error = np.mean(np.abs(signal_data_normalized - reconstructed_signal))\n",
    "\n",
    "print(f\"Error de reconstrucción: {reconstruction_error}\")\n",
    "# autoencoder.save(\"autoencoder_sismico.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra: Para mayor compresión se utiliza el Algoritmo de Hoffman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando la capa 1 (dense_4):\n",
      "Matriz de pesos cuantificada y aplanada:\n",
      "[144 194  63 ... 162  53 158]\n",
      "Tamaño de la señal original (bits): 18432000\n",
      "Tamaño de la señal comprimida (bits): 18220101\n",
      "Porcentaje de compresión: 98.8504%\n",
      "Son iguales la señal decodificada y la original cuantificada: Sí\n",
      "\n",
      "Procesando la capa 2 (dense_5):\n",
      "Matriz de pesos cuantificada y aplanada:\n",
      "[170 105  45 ... 187 203  46]\n",
      "Tamaño de la señal original (bits): 18432000\n",
      "Tamaño de la señal comprimida (bits): 18020034\n",
      "Porcentaje de compresión: 97.7649%\n",
      "Son iguales la señal decodificada y la original cuantificada: Sí\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "from collections import defaultdict\n",
    "\n",
    "# Cuantización de la matriz\n",
    "def quantize_signal(signal, num_levels=256):\n",
    "    min_val = np.min(signal)\n",
    "    max_val = np.max(signal)\n",
    "    # Normalizar y escalar a un rango de [0, num_levels-1]\n",
    "    quantized_signal = np.round((signal - min_val) / (max_val - min_val) * (num_levels - 1))\n",
    "    return quantized_signal.astype(int)\n",
    "\n",
    "# Aplanar la matriz de pesos\n",
    "def flatten_weights(weights):\n",
    "    return np.array(weights).flatten()\n",
    "\n",
    "# Vamos a crear la clase del árbol de Huffman\n",
    "class HuffmanNode:\n",
    "    def __init__(self, symbol=None, freq=0, left=None, right=None):\n",
    "        self.symbol = symbol\n",
    "        self.freq = freq\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.freq < other.freq\n",
    "\n",
    "# Árbol con las frecuencias\n",
    "def build_huffman_tree(symbols_freq):\n",
    "    heap = [HuffmanNode(symbol, freq) for symbol, freq in symbols_freq.items()]\n",
    "    heapq.heapify(heap)\n",
    "\n",
    "    while len(heap) > 1:\n",
    "        left = heapq.heappop(heap)\n",
    "        right = heapq.heappop(heap)\n",
    "        merged = HuffmanNode(None, left.freq + right.freq, left, right)\n",
    "        heapq.heappush(heap, merged)\n",
    "\n",
    "    return heap[0]\n",
    "\n",
    "# Generar el código Huffman\n",
    "def generate_huffman_codes(node, prefix=\"\", codebook={}):\n",
    "    if node.symbol is not None:\n",
    "        codebook[node.symbol] = prefix\n",
    "    else:\n",
    "        generate_huffman_codes(node.left, prefix + \"0\", codebook)\n",
    "        generate_huffman_codes(node.right, prefix + \"1\", codebook)\n",
    "    return codebook\n",
    "\n",
    "# Compresión de la señal usando los códigos de Huffman\n",
    "def huffman_encode(signal, codebook):\n",
    "    encoded_signal = ''.join([codebook[val] for val in signal])\n",
    "    return encoded_signal\n",
    "\n",
    "# Decodificación del Huffman\n",
    "def huffman_decode(encoded_signal, huffman_tree):\n",
    "    decoded_signal = []\n",
    "    current_node = huffman_tree\n",
    "    for bit in encoded_signal:\n",
    "        if bit == '0':\n",
    "            current_node = current_node.left\n",
    "        else:\n",
    "            current_node = current_node.right\n",
    "\n",
    "        if current_node.left is None and current_node.right is None:\n",
    "            decoded_signal.append(current_node.symbol)\n",
    "            current_node = huffman_tree  # Volvemos a la raíz\n",
    "\n",
    "    return np.array(decoded_signal)\n",
    "\n",
    "# Aplicación del algoritmo en la matriz de pesos del autoencoder\n",
    "# Aquí asumimos que tienes una lista de capas con pesos y sesgos\n",
    "for i, layer in enumerate(autoencoder.layers):\n",
    "    weights = layer.get_weights()\n",
    "    if len(weights) > 0:\n",
    "        print(f\"\\nProcesando la capa {i} ({layer.name}):\")\n",
    "        # Cuantificar y aplanar la matriz de pesos\n",
    "        quantized_weights = quantize_signal(flatten_weights(weights[0]))  # Solo procesamos los pesos\n",
    "        print(f\"Matriz de pesos cuantificada y aplanada:\\n{quantized_weights}\")\n",
    "\n",
    "        # Contar las frecuencias de los símbolos en la señal cuantizada\n",
    "        symbol_freq = defaultdict(int)\n",
    "        for value in quantized_weights:\n",
    "            symbol_freq[value] += 1\n",
    "\n",
    "        # Construir el árbol de Huffman\n",
    "        huffman_tree = build_huffman_tree(symbol_freq)\n",
    "\n",
    "        # Generar los códigos de Huffman\n",
    "        huffman_codes = generate_huffman_codes(huffman_tree)\n",
    "\n",
    "        # Comprimir la señal cuantizada\n",
    "        compressed_signal = huffman_encode(quantized_weights, huffman_codes)\n",
    "\n",
    "        # Tamaños de la señal original y comprimida\n",
    "        original_size = len(quantized_weights) * 8  # 8 bits por valor original\n",
    "        compressed_size = len(compressed_signal)\n",
    "        \n",
    "        print(f\"Tamaño de la señal original (bits): {original_size}\")\n",
    "        print(f\"Tamaño de la señal comprimida (bits): {compressed_size}\")\n",
    "        print(f\"Porcentaje de compresión: {(compressed_size / original_size) * 100:.4f}%\")\n",
    "\n",
    "        # Decodificar la señal comprimida\n",
    "        decoded_signal = huffman_decode(compressed_signal, huffman_tree)\n",
    "        is_identical = np.array_equal(decoded_signal, quantized_weights)\n",
    "        print(f\"Son iguales la señal decodificada y la original cuantificada: {'Sí' if is_identical else 'No'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
